{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/s3/opengptx/models/\n",
      "Getting model:  unsloth/Llama-3.3-70B-Instruct-GGUF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 14 files:   7%|▋         | 1/14 [22:21<4:50:39, 1341.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading model unsloth/Llama-3.3-70B-Instruct-GGUF: Consistency check failed: file should be of size 39758728992 but has size 15470315731 ((…)3.3-70B-Instruct-F16-00001-of-00004.gguf).\n",
      "We are sorry for the inconvenience. Please retry with `force_download=True`.\n",
      "If the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub.\n",
      "\n",
      "Getting model:  unsloth/Qwen2.5-72B-Instruct-bnb-4bit\n",
      "Error downloading model unsloth/Qwen2.5-72B-Instruct-bnb-4bit: [Errno 28] No space left on device: '/raid/s3/opengptx/models/hub/models--unsloth--Qwen2.5-72B-Instruct-bnb-4bit'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "print(os.environ.get(\"HF_HOME\"))\n",
    "\n",
    "# singel node setups for MN5!\n",
    "# bf16 or fp32\n",
    "# check throughput of different models across all languages (prompt)\n",
    "models = [\n",
    "    #\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    #\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    #\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "]\n",
    "\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "for model_name in models:\n",
    "    print(\"Getting model: \", model_name)\n",
    "    try:\n",
    "        local_dir = snapshot_download(repo_id=model_name)\n",
    "        print(f\"Model cached at: {local_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading model {model_name}: {e}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_filter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
