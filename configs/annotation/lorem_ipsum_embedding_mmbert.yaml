dataset_name: training
glob_pattern: "**/*.jsonl" #"*.jsonl"
input_dir: /raid/s3/opengptx/abbas/processed_data_natural/${dataset_name}_set
output_dir: /raid/s3/opengptx/jude/repos/ml_filter/embedding_ablations/mmbert_results/embeddings
embedding_dir: ${dataset_name}_embeddings
csv_hashmap_path: /raid/s3/opengptx/jude/repos/ml_filter/embedding_ablations/dataset/hashes/${dataset_name}.csv

embedding_model: jhu-clsp/mmBERT-base
batch_size: 128
writer_batch_size: 1000

# Whether to copy score into label and persist labels in output artifacts
save_labels: true

# Tokenization / embedding parameters
# These will be forwarded to the embedder.embed() method
max_length: 8192 #32768 #8192        # maximum sequence length for tokenization (adjust as needed)
padding: true           # whether to pad shorter sequences
truncation: true        # whether to truncate sequences longer than max_length

hdf5_dataset_name: train

tasks: 1 # number of tasks
workers: 1 
local_tasks: 1 # number of gpus in a node
local_rank_offset: 0
