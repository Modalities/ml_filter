#Points to an endpoint where the model is running

settings:
  model_name: meta-llama/Llama-3.2-3B-Instruct
  # we need to set this here manually as this is specified only when hosting the model
  num_gpus: 1
  tokenizer_name_or_path: ${settings.model_name}
  paths:
    raw_data_file_paths: 
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2014-41/da/00135.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2014-41/sv/00088.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2014-41/fr/00007.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2014-41/pt/00175.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2014-41/ga/00115.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2014-41/hu/00090.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2014-41/sk/00097.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2023-50/pl/00071.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2023-50/ca/00147.jsonl
    - /raid/s3/opengptx/eurolingua/cc_debug_datasets/cc_debug_subset_100_docs_raw/2023-50/is/00182.jsonl
    output_directory_path: data/output
    prompt_template_file_path: data/prompts/fineweb_edu/educational_prompt.yaml

llm_rest_client:
  model_name: ${settings.model_name}
  max_tokens: 8192
  max_new_tokens: 500
  temperature: 0.7
  max_pool_connections: 1000
  max_pool_maxsize: 1000 #TODO
  max_retries: 2
  backoff_factor: 0.4 
  timeout: 100
  verbose: false
  num_return_sequences: 5
  top_p: 0.9
  num_gpus: ${settings.num_gpus}

tokenizer:
  pretrained_model_name_or_path: ${settings.tokenizer_name_or_path}
  special_tokens: null
  add_generation_prompt: false

prompt_builder:
  prompt_template_file_path: ${settings.paths.prompt_template_file_path}
  # The maximum length of the prompt
  # Note that the maximum number of generated tokens is
  # model_context_length - max_prompt_length
  max_prompt_length: ${eval:'${llm_rest_client.max_tokens} - ${llm_rest_client.max_new_tokens}'}

document_processor:
  output_directory_path: ${settings.paths.output_directory_path}
  queue_size: 1000
  num_processes: 10
  score_metric_name: educational_score
  strings_to_remove: []
  jq_language_pattern: .metadata.language
